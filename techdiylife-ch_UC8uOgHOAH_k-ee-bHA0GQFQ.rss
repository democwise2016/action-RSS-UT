<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" xmlns:googleplay="http://www.google.com/schemas/play-podcasts/1.0" xmlns:media="http://www.rssboard.org/media-rss" version="2.0">
  <channel>
    <title><![CDATA[AI开发者-就爱瞎鼓捣[YT+]]]></title>
    <link>http://www.youtube.com/feeds/videos.xml?channel_id=UC8uOgHOAH_k-ee-bHA0GQFQ</link>
    <image>
      <url>https://yt3.googleusercontent.com/IE_8MkhRZYH8w0c-SIBy7gqPyXdn7GMrw9fIUMppAsjit-Smx29Zl-simURoP4avMjcvwd6xzQ=s900-b50-c-k-c0x008A95A5-no-rj</url>
      <title>AI开发者-就爱瞎鼓捣[YT+]</title>
      <link>http://www.youtube.com/feeds/videos.xml?channel_id=UC8uOgHOAH_k-ee-bHA0GQFQ</link>
    </image>
    <language>en-us</language>
    <atom:link href="https://www.youtube.com/feeds/videos.xml?channel_id=UC8uOgHOAH_k-ee-bHA0GQFQ" rel="self" type="application/rss+xml"/>
    <copyright><![CDATA[AI开发者-就爱瞎鼓捣[YT+]]]></copyright>
    <itunes:author><![CDATA[AI开发者-就爱瞎鼓捣[YT+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[
      <a href="https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ" target="_blank">https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ</a><br />
<br />
<a href="https://www.youtube.com/feeds/videos.xml?channel_id=UC8uOgHOAH_k-ee-bHA0GQFQ" target="_blank">https://www.youtube.com/feeds/videos.xml?channel_id=UC8uOgHOAH_k-ee-bHA0GQFQ</a>
      ]]>
    </itunes:summary>
    <description>
      <![CDATA[
      <a href="https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ" target="_blank">https://www.youtube.com/channel/UC8uOgHOAH_k-ee-bHA0GQFQ</a><br />
<br />
<a href="https://www.youtube.com/feeds/videos.xml?channel_id=UC8uOgHOAH_k-ee-bHA0GQFQ" target="_blank">https://www.youtube.com/feeds/videos.xml?channel_id=UC8uOgHOAH_k-ee-bHA0GQFQ</a>
      ]]>
    </description>
    <itunes:owner>
      <itunes:name><![CDATA[AI开发者-就爱瞎鼓捣[YT+]]]></itunes:name>
    </itunes:owner>
    <itunes:image href="https://yt3.googleusercontent.com/IE_8MkhRZYH8w0c-SIBy7gqPyXdn7GMrw9fIUMppAsjit-Smx29Zl-simURoP4avMjcvwd6xzQ=s900-b50-c-k-c0x008A95A5-no-rj"/>
<item>
      <title><![CDATA[Phi-4多模态大模型 VS 白宫吵架录音，能识别成功吗？]]></title>
      <link>https://www.youtube.com/watch?v=RIlbhip5xOc</link>
      <itunes:title><![CDATA[Phi-4多模态大模型 VS 白宫吵架录音，能识别成功吗？]]></itunes:title>
      <itunes:author><![CDATA[AI开发者-就爱瞎鼓捣]]></itunes:author>
      <itunes:summary>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">好的，以下是影片摘要，以多段落呈现：<h1>引言與模型介紹</h1>影片開頭，主持人向觀眾問候，並宣告將挑戰微軟的 Phi-4 Multimodal instruct 多模態大模型。此前，影片曾介紹 Phi-4 在語音辨識上的優異表現，特別是其在語音辨識的錯誤率上低於英偉達的 Canary 模型。主持人興致勃勃地決定用 Phi-4 來挑戰一項艱鉅的任務：辨識澤倫斯基與川普在白宮的吵架語音。由於川普經常打斷對方發言，這類吵架語音對 AI 語音辨識來說極具挑戰性。<h1>實驗準備：音訊擷取與處理</h1>為了進行實驗，主持人首先從 YouTube 上下載了澤倫斯基與川普吵架的影片音訊。他使用了 yt-dlp 工具下載影片，並用 FFmpeg 從影片中提取音訊。吵架片段的總長度為 4 分 53 秒。由於 Phi-4 模型每次只能處理 30 秒的語音，主持人使用 silero_vad 工具將音訊進行拆分，並保存分割後的語音檔案。<h1>對照組：OpenAI Whisper Large V3 模型</h1>為了進行比較，主持人也使用了 OpenAI 最強大的語音辨識模型 Whisper Large V3 來處理相同的音訊。他展示了使用 Whisper Large V3 的程式碼，首先導入模型，然後讀取先前分割好的語音片段進行處理。處理速度相當快，總共花費約 30 秒。<h1>Phi-4 模型實驗與速度比較</h1>接著，主持人使用 Phi-4 模型來處理這些語音。他同樣先導入模型，並設定相關提示詞。由於 Phi-4 模型的參數量是 Whisper 模型的三倍，處理速度相對較慢，花了約 70 秒才完成。<h1>結果分析：比較與評估</h1>主持人將 Whisper 和 Phi-4 模型的辨識結果進行比較。他強調，由於吵架語音的特殊性，辨識難度很高。他將在影片後段嵌入字幕，供觀眾自行比較。他指出，主要存在兩類問題：一是語音遺失，即原本有語音卻未被辨識出來；二是幻覺問題，即模型輸出了語音檔案中不存在的內容。<h1>具體分析：遺失與幻覺</h1>在遺失問題上，主持人舉例說明了澤倫斯基連續說「what」的情況，以及 Phi-4 模型未能辨識出某些句子。他指出，Whisper 模型約有 7 處遺失，而 Phi-4 模型只有 5 處，後者表現較佳。在幻覺問題上，主持人說明了 Whisper 模型在某些情況下，輸出了額外的內容，而 Phi-4 模型則沒有出現這種問題。他指出，幻覺問題是大語言模型和多模態模型常見的問題，而 Phi-4 在這方面表現較好，沒有出現幻覺。<h1>結論與總結</h1>主持人總結，從整體來看，Phi-4 模型優於 Whisper Large V3。 Phi-4 模型能夠更好地辨識語音內容，而 Whisper 模型則更容易產生幻覺。主持人邀請觀眾觀看影片的最後部分，那裏將展示兩個模型的辨識結果嵌入吵架影片中的情況。最後，他感謝觀眾的觀看，並鼓勵大家點讚、分享和訂閱。<hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">AI开发前途无限。欢迎来到AI开发者的频道。前面的视频呢。我们介绍了微软的Phi-4 Multimodal instruct。多模态大模型。前面我们讲过。这个模型。在语音识别上的性能是相当的不错的。上个视频我们发布的时候呢。这个排行榜还没有更新。这样你可以看到这个排行榜。已经更新了。平均的误识率呢是6.14。要低于英伟达的这个canary的这个模型。那既然这个模型这么强呢。所以我今天。就想用它来做一个挑战。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我准备试一下这个模型。看看它能不能识别。泽连斯基在美国白宫。与川普吵架的语音。不知道最近。你有没有看过。泽连斯基与川普吵架的视频。川普吵架的能力。那绝对是一流的。那不管对方在说什么。反正川普呢就是总是想办法打断对方。所以这种吵架的语音部分。对于AI语音识别来说呢。也是非常的困难。那下面呢。我们就来试试。看看这个微软号称最强的Phi4模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">看看他能做到什么程度。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/RIlbhip5xOc_61.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我在油管上呢找到了一个视频。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">然后呢。我先将这个音频文件呢下载下来。这里我使用这个yt-dlp这个工具。直接呢从油管上呢。先把这个视频下载下来。然后呢。再用Ffmpeg将这个视频中的语音呢。提取出来。泽连斯基与川普吵架的这部分。视频的总长度呢。是4分53秒。快字模型与whisper模型呢是一样的。单次呢他只能处理30秒的语音。所以呢这里需要我们。自己对这个语音呢进行拆分。这里呢我使用这个silero_vad这个工具呢。对语音呢进行一下拆分。然后呢是保存这些切分后的语音文件。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为了做对比呢。这次呢我还使用了这个open AI的。最强的语音识别模型。也就是whisper large v3。也来识别一下这些语音。这边呢。是我准备的这个使用whisper large V3的代码。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先呢这一部分是导入模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">然后呢。就是读取我们前面切分好的语音片段。分别进行处理。我们可以看到这个处理的速度呢。还是挺快的。这个语音的文件长度呢为4分53秒钟。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/RIlbhip5xOc_127.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后。使用这个whisper large V3模型处理完呢。一共用了大约30秒钟。然后我们再来使用这个phi4模型。来处理一下。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先呢也是把模型导入。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">然后呢就是设置相关的提示词。这部分呢也是处理前面准备好的语音。这个速度呢比whisper是要慢一些的。主要原因呢是这个模型的参数量呢。是whisper模型的三倍。我们可以看到。差不多呢是用了70秒钟才处理完成。好下面我们就来比较一下这个结果。左边呢是whisper的结果。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">右边呢是phi4的结果。首先呢。先给大家介绍一下我比较的一些情况。在这个视频的最后呢。我会把这两个字幕呢。再嵌入到这个吵架的视频当中。大家呢可以自己再去对比一下。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我比较了这两个识别结果呢。与这个语音。然后呢对一些错误呢做了一些标记。整体上来说呢。这个吵架的语音识别难。度还是挺大的。这两个模型。识别的还算可以。这里面呢。主要有两类问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其中一类问题呢。就是丢失。也就是本来是有语音的。但是呢没有识别出来。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/RIlbhip5xOc_194.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如这个位置。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这里呢泽连斯基呢。连续说了三个what what what。但是呢我们可以看到whisper模型。并没有识别这三个what。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另外呢这一部分 JD you speaking about呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">phi4模型呢。也没有识别出来。这三个what呢。不识别呢。还可以说得通的话呢。但是phi4丢失的这一部分呢。还是应该被识别出来的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">整个识别结果中呢。whisper呢大约有7处丢失的地方。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而Fi四模型呢。只有5处。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">从这个问题点上来看呢。这个phi4的模型。比whisper呢是要好一些的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二类问题呢。就是幻觉问题。这里所谓的幻觉呢。就是这个语音文件当中。并没有讲这部分内容。但是模型呢。却输出了内容。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如说这个but if you。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">之后呢泽连斯基呢。就被打断了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是whisper模型呢。还是继续输出了一些内容。我们可以看一下phi4模型。它并没有输出额外的内容。这个幻觉问题。在这个whisper模型当中呢。它出现了7次。但是这个phi4模型。并没有出现这类问题。目前我们常见的大语言模型。或者说基于大语言模型的多模态模型。幻觉问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/RIlbhip5xOc_265.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实是一个非常常见的问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家呢也在努力的去解决这个问题。从这个结果对比来看呢。phi4模型。在这个方面做。的还是挺不错的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除了上面两类问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">还有一些识别的结果是挺难判断的。这个吵架吵的的确是太凶了。有很多地方具体该识别成什么。我也说不好。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如说这个第23段这个语音。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">whisper是直接放弃识别了。但是phi4呢。依然坚持输出。那大体上。看起来还可以。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">整体比较下来的感觉呢。phi4是要强于这个whisper large V3的模型的。从语音识别的角度来看呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">phi4模型呢。它是可以比较好的。按照这种语音中的内容进行识别。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">相对应的呢。whisper模型。就会经常产生一些幻觉的问题。好以上呢。就是这两个模型的比较。最后一部分视频呢。就是我讲这两个识别结果。都嵌入到了三位总统吵架的视频里。大家呢可以体会一下。看完之后呢。也别忘了一键三连。</p>

<hr style="clear:both" />
====
<p>https://www.youtube.com/watch?v=RIlbhip5xOc</p><p>今天，我们用微软Phi-4大模型来挑战一个特殊任务——识别白宫的一段激烈争吵录音。这款AI语音识别能力到底有多强？它能准确还原内容吗？</p>]]>
      </itunes:summary>
      <description>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">好的，以下是影片摘要，以多段落呈现：<h1>引言與模型介紹</h1>影片開頭，主持人向觀眾問候，並宣告將挑戰微軟的 Phi-4 Multimodal instruct 多模態大模型。此前，影片曾介紹 Phi-4 在語音辨識上的優異表現，特別是其在語音辨識的錯誤率上低於英偉達的 Canary 模型。主持人興致勃勃地決定用 Phi-4 來挑戰一項艱鉅的任務：辨識澤倫斯基與川普在白宮的吵架語音。由於川普經常打斷對方發言，這類吵架語音對 AI 語音辨識來說極具挑戰性。<h1>實驗準備：音訊擷取與處理</h1>為了進行實驗，主持人首先從 YouTube 上下載了澤倫斯基與川普吵架的影片音訊。他使用了 yt-dlp 工具下載影片，並用 FFmpeg 從影片中提取音訊。吵架片段的總長度為 4 分 53 秒。由於 Phi-4 模型每次只能處理 30 秒的語音，主持人使用 silero_vad 工具將音訊進行拆分，並保存分割後的語音檔案。<h1>對照組：OpenAI Whisper Large V3 模型</h1>為了進行比較，主持人也使用了 OpenAI 最強大的語音辨識模型 Whisper Large V3 來處理相同的音訊。他展示了使用 Whisper Large V3 的程式碼，首先導入模型，然後讀取先前分割好的語音片段進行處理。處理速度相當快，總共花費約 30 秒。<h1>Phi-4 模型實驗與速度比較</h1>接著，主持人使用 Phi-4 模型來處理這些語音。他同樣先導入模型，並設定相關提示詞。由於 Phi-4 模型的參數量是 Whisper 模型的三倍，處理速度相對較慢，花了約 70 秒才完成。<h1>結果分析：比較與評估</h1>主持人將 Whisper 和 Phi-4 模型的辨識結果進行比較。他強調，由於吵架語音的特殊性，辨識難度很高。他將在影片後段嵌入字幕，供觀眾自行比較。他指出，主要存在兩類問題：一是語音遺失，即原本有語音卻未被辨識出來；二是幻覺問題，即模型輸出了語音檔案中不存在的內容。<h1>具體分析：遺失與幻覺</h1>在遺失問題上，主持人舉例說明了澤倫斯基連續說「what」的情況，以及 Phi-4 模型未能辨識出某些句子。他指出，Whisper 模型約有 7 處遺失，而 Phi-4 模型只有 5 處，後者表現較佳。在幻覺問題上，主持人說明了 Whisper 模型在某些情況下，輸出了額外的內容，而 Phi-4 模型則沒有出現這種問題。他指出，幻覺問題是大語言模型和多模態模型常見的問題，而 Phi-4 在這方面表現較好，沒有出現幻覺。<h1>結論與總結</h1>主持人總結，從整體來看，Phi-4 模型優於 Whisper Large V3。 Phi-4 模型能夠更好地辨識語音內容，而 Whisper 模型則更容易產生幻覺。主持人邀請觀眾觀看影片的最後部分，那裏將展示兩個模型的辨識結果嵌入吵架影片中的情況。最後，他感謝觀眾的觀看，並鼓勵大家點讚、分享和訂閱。<hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">AI开发前途无限。欢迎来到AI开发者的频道。前面的视频呢。我们介绍了微软的Phi-4 Multimodal instruct。多模态大模型。前面我们讲过。这个模型。在语音识别上的性能是相当的不错的。上个视频我们发布的时候呢。这个排行榜还没有更新。这样你可以看到这个排行榜。已经更新了。平均的误识率呢是6.14。要低于英伟达的这个canary的这个模型。那既然这个模型这么强呢。所以我今天。就想用它来做一个挑战。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我准备试一下这个模型。看看它能不能识别。泽连斯基在美国白宫。与川普吵架的语音。不知道最近。你有没有看过。泽连斯基与川普吵架的视频。川普吵架的能力。那绝对是一流的。那不管对方在说什么。反正川普呢就是总是想办法打断对方。所以这种吵架的语音部分。对于AI语音识别来说呢。也是非常的困难。那下面呢。我们就来试试。看看这个微软号称最强的Phi4模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">看看他能做到什么程度。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/RIlbhip5xOc_61.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我在油管上呢找到了一个视频。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">然后呢。我先将这个音频文件呢下载下来。这里我使用这个yt-dlp这个工具。直接呢从油管上呢。先把这个视频下载下来。然后呢。再用Ffmpeg将这个视频中的语音呢。提取出来。泽连斯基与川普吵架的这部分。视频的总长度呢。是4分53秒。快字模型与whisper模型呢是一样的。单次呢他只能处理30秒的语音。所以呢这里需要我们。自己对这个语音呢进行拆分。这里呢我使用这个silero_vad这个工具呢。对语音呢进行一下拆分。然后呢是保存这些切分后的语音文件。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为了做对比呢。这次呢我还使用了这个open AI的。最强的语音识别模型。也就是whisper large v3。也来识别一下这些语音。这边呢。是我准备的这个使用whisper large V3的代码。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先呢这一部分是导入模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">然后呢。就是读取我们前面切分好的语音片段。分别进行处理。我们可以看到这个处理的速度呢。还是挺快的。这个语音的文件长度呢为4分53秒钟。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/RIlbhip5xOc_127.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后。使用这个whisper large V3模型处理完呢。一共用了大约30秒钟。然后我们再来使用这个phi4模型。来处理一下。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先呢也是把模型导入。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">然后呢就是设置相关的提示词。这部分呢也是处理前面准备好的语音。这个速度呢比whisper是要慢一些的。主要原因呢是这个模型的参数量呢。是whisper模型的三倍。我们可以看到。差不多呢是用了70秒钟才处理完成。好下面我们就来比较一下这个结果。左边呢是whisper的结果。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">右边呢是phi4的结果。首先呢。先给大家介绍一下我比较的一些情况。在这个视频的最后呢。我会把这两个字幕呢。再嵌入到这个吵架的视频当中。大家呢可以自己再去对比一下。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我比较了这两个识别结果呢。与这个语音。然后呢对一些错误呢做了一些标记。整体上来说呢。这个吵架的语音识别难。度还是挺大的。这两个模型。识别的还算可以。这里面呢。主要有两类问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其中一类问题呢。就是丢失。也就是本来是有语音的。但是呢没有识别出来。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/RIlbhip5xOc_194.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如这个位置。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这里呢泽连斯基呢。连续说了三个what what what。但是呢我们可以看到whisper模型。并没有识别这三个what。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另外呢这一部分 JD you speaking about呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">phi4模型呢。也没有识别出来。这三个what呢。不识别呢。还可以说得通的话呢。但是phi4丢失的这一部分呢。还是应该被识别出来的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">整个识别结果中呢。whisper呢大约有7处丢失的地方。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而Fi四模型呢。只有5处。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">从这个问题点上来看呢。这个phi4的模型。比whisper呢是要好一些的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二类问题呢。就是幻觉问题。这里所谓的幻觉呢。就是这个语音文件当中。并没有讲这部分内容。但是模型呢。却输出了内容。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如说这个but if you。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">之后呢泽连斯基呢。就被打断了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是whisper模型呢。还是继续输出了一些内容。我们可以看一下phi4模型。它并没有输出额外的内容。这个幻觉问题。在这个whisper模型当中呢。它出现了7次。但是这个phi4模型。并没有出现这类问题。目前我们常见的大语言模型。或者说基于大语言模型的多模态模型。幻觉问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/RIlbhip5xOc_265.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实是一个非常常见的问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家呢也在努力的去解决这个问题。从这个结果对比来看呢。phi4模型。在这个方面做。的还是挺不错的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除了上面两类问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">还有一些识别的结果是挺难判断的。这个吵架吵的的确是太凶了。有很多地方具体该识别成什么。我也说不好。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如说这个第23段这个语音。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">whisper是直接放弃识别了。但是phi4呢。依然坚持输出。那大体上。看起来还可以。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">整体比较下来的感觉呢。phi4是要强于这个whisper large V3的模型的。从语音识别的角度来看呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">phi4模型呢。它是可以比较好的。按照这种语音中的内容进行识别。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">相对应的呢。whisper模型。就会经常产生一些幻觉的问题。好以上呢。就是这两个模型的比较。最后一部分视频呢。就是我讲这两个识别结果。都嵌入到了三位总统吵架的视频里。大家呢可以体会一下。看完之后呢。也别忘了一键三连。</p>

<hr style="clear:both" />
====
<p>https://www.youtube.com/watch?v=RIlbhip5xOc</p><p>今天，我们用微软Phi-4大模型来挑战一个特殊任务——识别白宫的一段激烈争吵录音。这款AI语音识别能力到底有多强？它能准确还原内容吗？</p>]]>
      </description>
      <content:encoded><![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">好的，以下是影片摘要，以多段落呈现：<h1>引言與模型介紹</h1>影片開頭，主持人向觀眾問候，並宣告將挑戰微軟的 Phi-4 Multimodal instruct 多模態大模型。此前，影片曾介紹 Phi-4 在語音辨識上的優異表現，特別是其在語音辨識的錯誤率上低於英偉達的 Canary 模型。主持人興致勃勃地決定用 Phi-4 來挑戰一項艱鉅的任務：辨識澤倫斯基與川普在白宮的吵架語音。由於川普經常打斷對方發言，這類吵架語音對 AI 語音辨識來說極具挑戰性。<h1>實驗準備：音訊擷取與處理</h1>為了進行實驗，主持人首先從 YouTube 上下載了澤倫斯基與川普吵架的影片音訊。他使用了 yt-dlp 工具下載影片，並用 FFmpeg 從影片中提取音訊。吵架片段的總長度為 4 分 53 秒。由於 Phi-4 模型每次只能處理 30 秒的語音，主持人使用 silero_vad 工具將音訊進行拆分，並保存分割後的語音檔案。<h1>對照組：OpenAI Whisper Large V3 模型</h1>為了進行比較，主持人也使用了 OpenAI 最強大的語音辨識模型 Whisper Large V3 來處理相同的音訊。他展示了使用 Whisper Large V3 的程式碼，首先導入模型，然後讀取先前分割好的語音片段進行處理。處理速度相當快，總共花費約 30 秒。<h1>Phi-4 模型實驗與速度比較</h1>接著，主持人使用 Phi-4 模型來處理這些語音。他同樣先導入模型，並設定相關提示詞。由於 Phi-4 模型的參數量是 Whisper 模型的三倍，處理速度相對較慢，花了約 70 秒才完成。<h1>結果分析：比較與評估</h1>主持人將 Whisper 和 Phi-4 模型的辨識結果進行比較。他強調，由於吵架語音的特殊性，辨識難度很高。他將在影片後段嵌入字幕，供觀眾自行比較。他指出，主要存在兩類問題：一是語音遺失，即原本有語音卻未被辨識出來；二是幻覺問題，即模型輸出了語音檔案中不存在的內容。<h1>具體分析：遺失與幻覺</h1>在遺失問題上，主持人舉例說明了澤倫斯基連續說「what」的情況，以及 Phi-4 模型未能辨識出某些句子。他指出，Whisper 模型約有 7 處遺失，而 Phi-4 模型只有 5 處，後者表現較佳。在幻覺問題上，主持人說明了 Whisper 模型在某些情況下，輸出了額外的內容，而 Phi-4 模型則沒有出現這種問題。他指出，幻覺問題是大語言模型和多模態模型常見的問題，而 Phi-4 在這方面表現較好，沒有出現幻覺。<h1>結論與總結</h1>主持人總結，從整體來看，Phi-4 模型優於 Whisper Large V3。 Phi-4 模型能夠更好地辨識語音內容，而 Whisper 模型則更容易產生幻覺。主持人邀請觀眾觀看影片的最後部分，那裏將展示兩個模型的辨識結果嵌入吵架影片中的情況。最後，他感謝觀眾的觀看，並鼓勵大家點讚、分享和訂閱。<hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">AI开发前途无限。欢迎来到AI开发者的频道。前面的视频呢。我们介绍了微软的Phi-4 Multimodal instruct。多模态大模型。前面我们讲过。这个模型。在语音识别上的性能是相当的不错的。上个视频我们发布的时候呢。这个排行榜还没有更新。这样你可以看到这个排行榜。已经更新了。平均的误识率呢是6.14。要低于英伟达的这个canary的这个模型。那既然这个模型这么强呢。所以我今天。就想用它来做一个挑战。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我准备试一下这个模型。看看它能不能识别。泽连斯基在美国白宫。与川普吵架的语音。不知道最近。你有没有看过。泽连斯基与川普吵架的视频。川普吵架的能力。那绝对是一流的。那不管对方在说什么。反正川普呢就是总是想办法打断对方。所以这种吵架的语音部分。对于AI语音识别来说呢。也是非常的困难。那下面呢。我们就来试试。看看这个微软号称最强的Phi4模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">看看他能做到什么程度。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/RIlbhip5xOc_61.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我在油管上呢找到了一个视频。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">然后呢。我先将这个音频文件呢下载下来。这里我使用这个yt-dlp这个工具。直接呢从油管上呢。先把这个视频下载下来。然后呢。再用Ffmpeg将这个视频中的语音呢。提取出来。泽连斯基与川普吵架的这部分。视频的总长度呢。是4分53秒。快字模型与whisper模型呢是一样的。单次呢他只能处理30秒的语音。所以呢这里需要我们。自己对这个语音呢进行拆分。这里呢我使用这个silero_vad这个工具呢。对语音呢进行一下拆分。然后呢是保存这些切分后的语音文件。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为了做对比呢。这次呢我还使用了这个open AI的。最强的语音识别模型。也就是whisper large v3。也来识别一下这些语音。这边呢。是我准备的这个使用whisper large V3的代码。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先呢这一部分是导入模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">然后呢。就是读取我们前面切分好的语音片段。分别进行处理。我们可以看到这个处理的速度呢。还是挺快的。这个语音的文件长度呢为4分53秒钟。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/RIlbhip5xOc_127.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后。使用这个whisper large V3模型处理完呢。一共用了大约30秒钟。然后我们再来使用这个phi4模型。来处理一下。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先呢也是把模型导入。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">然后呢就是设置相关的提示词。这部分呢也是处理前面准备好的语音。这个速度呢比whisper是要慢一些的。主要原因呢是这个模型的参数量呢。是whisper模型的三倍。我们可以看到。差不多呢是用了70秒钟才处理完成。好下面我们就来比较一下这个结果。左边呢是whisper的结果。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">右边呢是phi4的结果。首先呢。先给大家介绍一下我比较的一些情况。在这个视频的最后呢。我会把这两个字幕呢。再嵌入到这个吵架的视频当中。大家呢可以自己再去对比一下。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我比较了这两个识别结果呢。与这个语音。然后呢对一些错误呢做了一些标记。整体上来说呢。这个吵架的语音识别难。度还是挺大的。这两个模型。识别的还算可以。这里面呢。主要有两类问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其中一类问题呢。就是丢失。也就是本来是有语音的。但是呢没有识别出来。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/RIlbhip5xOc_194.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如这个位置。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这里呢泽连斯基呢。连续说了三个what what what。但是呢我们可以看到whisper模型。并没有识别这三个what。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另外呢这一部分 JD you speaking about呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">phi4模型呢。也没有识别出来。这三个what呢。不识别呢。还可以说得通的话呢。但是phi4丢失的这一部分呢。还是应该被识别出来的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">整个识别结果中呢。whisper呢大约有7处丢失的地方。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而Fi四模型呢。只有5处。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">从这个问题点上来看呢。这个phi4的模型。比whisper呢是要好一些的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二类问题呢。就是幻觉问题。这里所谓的幻觉呢。就是这个语音文件当中。并没有讲这部分内容。但是模型呢。却输出了内容。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如说这个but if you。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">之后呢泽连斯基呢。就被打断了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是whisper模型呢。还是继续输出了一些内容。我们可以看一下phi4模型。它并没有输出额外的内容。这个幻觉问题。在这个whisper模型当中呢。它出现了7次。但是这个phi4模型。并没有出现这类问题。目前我们常见的大语言模型。或者说基于大语言模型的多模态模型。幻觉问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/RIlbhip5xOc_265.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实是一个非常常见的问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家呢也在努力的去解决这个问题。从这个结果对比来看呢。phi4模型。在这个方面做。的还是挺不错的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除了上面两类问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">还有一些识别的结果是挺难判断的。这个吵架吵的的确是太凶了。有很多地方具体该识别成什么。我也说不好。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如说这个第23段这个语音。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">whisper是直接放弃识别了。但是phi4呢。依然坚持输出。那大体上。看起来还可以。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">整体比较下来的感觉呢。phi4是要强于这个whisper large V3的模型的。从语音识别的角度来看呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">phi4模型呢。它是可以比较好的。按照这种语音中的内容进行识别。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">相对应的呢。whisper模型。就会经常产生一些幻觉的问题。好以上呢。就是这两个模型的比较。最后一部分视频呢。就是我讲这两个识别结果。都嵌入到了三位总统吵架的视频里。大家呢可以体会一下。看完之后呢。也别忘了一键三连。</p>

<hr style="clear:both" />
====
<p>https://www.youtube.com/watch?v=RIlbhip5xOc</p><p>今天，我们用微软Phi-4大模型来挑战一个特殊任务——识别白宫的一段激烈争吵录音。这款AI语音识别能力到底有多强？它能准确还原内容吗？</p>]]></content:encoded>
      <itunes:image href="https://i.ytimg.com/vi/RIlbhip5xOc/hqdefault.jpg"/>
      <pubDate>2025-03-06T14:50:55.000Z</pubDate>
    </item><item>
      <title><![CDATA[微软新模型炸场！超越英伟达，登顶语音识别榜！]]></title>
      <link>https://www.youtube.com/watch?v=RZAJaKL-pW4</link>
      <itunes:title><![CDATA[微软新模型炸场！超越英伟达，登顶语音识别榜！]]></itunes:title>
      <itunes:author><![CDATA[AI开发者-就爱瞎鼓捣]]></itunes:author>
      <itunes:summary>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><h1>介紹微软 Phi-4 多模態模型</h1>這部影片介紹了微軟最新發布的 Phi-4 Multi model instruct 模型，一款輕量級、開放的多模態基礎模型。影片重點在於其在語音任務上的優異表現，特別是在Huggingface的語音識別模型排行榜上，超越了NVIDIA的Canary 1B 模型。<h1>模型概述</h1>Phi-4 是一個輕量級的多模態基礎模型，基於 Phi3.5 和 Phi4 版本開發。它支援文本、圖像和音訊輸入，並可以生成文本輸出，上下文長度可達 128K tokens。模型的訓練採用了目前常用的技術。在多模態支援語言方面，文本支援多種語言，視覺僅支援英文，音訊則支援英文、中文等 8 種語言。<h1>應用場景與特色</h1>由於 Phi-4 的參數量只有 5.6B，因此更適合計算環境受限或低延遲的場景。 儘管參數量不大，但其能力毫不遜色，包括強大的推理能力（數學和邏輯推理）、工具調用支援、圖像處理能力（圖像理解、OCR 處理、多圖像比較）和音訊處理能力（語音識別、語音問答、翻譯，甚至直接生成語音摘要）。<h1>模型改進與性能</h1>Phi-4 是基於 Phi3 系列開發的版本，主要改進包括新架構設計、更大的詞彙表、多語言和多模態支援。 在性能方面，尤其是在語音識別和翻譯方面，Phi-4 的表現優異。 語音識別誤識率低於 Qwen2-Audio 和 OpenAI 的 Whisper V3。 在語音翻譯方面，性能也超越了其他模型。此外，Phi-4 還支援視覺和語音的同步輸入，在多個測試基準中表現出色。<h1>模型訓練細節</h1>Phi-4 的訓練使用了 512 張 A100 進行，訓練時間為 28 天。 訓練資料包括 5T tokens 的文本資料、230 萬小時的語音資料，以及 1.1T 的圖像和文本對齊資料。 影片中也介紹了訓練數據的組成以及數據的篩選和處理方式。<h1>模型使用示範</h1>為了使用 Phi-4 模型，首先需要安裝運行環境並安裝相關的開發庫。 影片演示了如何下載模型並加載到本地機器上，模型的顯存占用約為 13GB。 接下來，影片展示了語音識別和翻譯的測試，使用微軟官方的例子，將語音識別為文本，然後翻譯成中文，並在識別文本和翻譯結果之間加上 Sep 標記。<h1>語音識別能力測試</h1>為了測試 Phi-4 的語音識別能力，影片使用了多人對話的語音進行測試，結果顯示模型可以準確識別，甚至可以正確識別兩個說話者交叉的部分。 此外，影片還測試了使用剪映提供的語音識別功能時，在辨識一些英文單字時經常出現錯誤的影片，Phi-4 的表現要好得多，除了 OpenR1 辨識錯誤外，其他地方基本準確，相較於剪映，Phi-4 的識別效果更佳。<h1>圖像理解能力測試</h1>影片還演示了圖像理解的例子，詢問模型圖像中有什麼內容。 雖然模型對圖像的描述理解能力有待提高，但總體來說，Phi-4 的語音識別能力優於剪映提供的語音識別功能。 尤其是在模型只有 5.6B 參數，正常使用僅需 13GB 顯存的情況下，對於有語音識別和翻譯需求的使用者，推薦測試此模型。<hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">AI开发前途无限。欢迎来到AI开发者的频道。今天这个视频呢。我们介绍一下。微软这周刚刚发布的一款最新的模型。也就是这个Phi-4 Multi model instruct模型。这是一款轻量级的开放的。多模态的基础模型。之所以介绍这款模型呢。主要是因为这款模型。在语音任务上的表现。可以说是非常的棒。它在Huggingface的。语音识别模型排行榜上呢。超过了当前排名第一的。英伟达的Canary 1B的模型。下面呢首先我会介绍一下这款模型。然后呢。再来介绍一下如何使用这个模型。并来测试一下它的语音识别能力。好首先我们先来看一下这个模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那这个模型呢。它是一个轻量级的多模态基础模型。它是在Phi3.5和Phi4版本的基础上的。来建立起来的。这款模型呢。它支持处理文本。图像和音频的输入。并可以生成文本的输出。它支持呢128K TOKEN的上下文。模型的训练呢。采用的这些技术呢。是当前最常用的一些技术。这里呢。是它各个模态支持的语言的情况。文本呢。我们可以看到它支持的语言呢。还是挺多的。视觉呢只支持英文。音频呢支持英文。中文等8种语言。然后呢是微软介绍的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个模型可以适用的应用场景。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/RZAJaKL-pW4_79.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先呢。这个模型的参数量呢只有5.6个B。所以呢。它更适合于这种计算环境受限。或者低延迟的这种场景。虽然参数量不大。但是他的能力呢。一点也不差。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如他的强推理能力。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">也就是说他的数学。和逻辑推理能力呢是不错的。然后呢他还可以支持呢使用工具调用。下面呢就是它的这个图像处理能力。包括图像理解。OCR的处理。多图像的对比等等。最后呢是它的这个音频的处理能力。包括语音识别。语音的问答翻译。甚至呢可以对语音做直接的摘要。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三点呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是当前这个开放版本的说明。这里他讲呢。他这个版本呢是基于Phi3系列来建立的。相比Phi3系列呢。它主要的改进呢有新架构的设计。更大的词汇表。多语言和多模态的支持等等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第四点呢是模型的性能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">也就是呢。这个模型在公开测试集上的表现。到底如何。首先呢。我们先看一下它的这个语音能力。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/RZAJaKL-pW4_142.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一点呢。就是说。它的这个语音识别和语音翻译的能力。是非常棒的。这个表呢是语音识别上的一个结果。通过这些结果我们可以看到。这个模型呢在语音识别上的误识率呢。低于Qwen2-Audio以及open AI的Whisper V3。在下面这个结果呢。是它在8种语言上的语音识别结果。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三个结果呢是语音翻译。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个结果呢是值越高呢性能越好。从这个结果也可以看到。微软的这款模型呢在语音翻译上呢。也超过了当前我们熟知的。这种性能比较好的模型。然后这款模型呢。还可以同时支持。这个视觉和语音的输入。这个结果呢。它是比较了。与当前支持音频和视觉输入的。最好的多模态模型的一个比较。从这个多个测试基准中呢我们可以看。到这个Phi-4多模态的模型。表现还是非常不错的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除了结果呢。微软呢还介绍了。这个模型的训练的一些相关的细节。这个模型训练呢。使用了512张A100进行训练的。训练时间呢是训练了28天。训练数据呢包括5T TOKEN的文本数据。230万小时的语音数据。和这个1.1T的图像和文本对齐数据。虽然说模型不大。但是这个数据量还是很大的。然后呢他还介绍了。他们的这个训练数据的组成。以及这个数据。是如何进行筛选和处理的。好以上内容呢。感兴趣的小伙伴呢。可以去自己读一下。这个模型的技术报告。最后呢。我们来介绍一下如何运行这个模型。要运行这个模型呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/RZAJaKL-pW4_242.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先呢我们需要安装一个运行环境。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">并根据他这的要求呢。安装相关的这种开发库。这部分呢。是我们导入这个相关的开发库。然后执行这一部分的代码呢。就可以将这个模型。下载到我们本地的机器上。这边呢我已经下载好了。点击这个执行之后呢。就可以看到这个模型呢被导入了。这个模型。导入之后呢可以看到这个显存占用呢。大约是在13个g左右。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先呢。我们来测一下它这个语音识别。以及翻译的能力。这个例子呢。是微软官方给出的一个例子。这边呢是一个语音文件。然后呢是这个模型的一个提示词。这个提示词呢是让这个模型呢。将这个语音呢识别为文本。然后呢再翻译为中文。并要求这个模型呢。在识别文本与翻译结果。之间呢加上Sep这个标记。然后这就是读取这个语音的文件。读取之后呢。是做TOKEN和数据的基本处理。然后就是进行这个文本的产生。好我们可以看到这边结果呢。就产生出来了。这上面呢还显示了他的这个提示词。以及这个模型产生的回答。我们可以看到。前面呢是语音识别的部分。Sep后面的部分呢。就是将上面的这个识别的文本。翻译为了中文。然后。为了测试一下它语音识别的能力呢。我找了一个多人对话的语音。我们可以先来听一下这段语音。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/RZAJaKL-pW4_337.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">你可以听到这个语音当中呢。它有多个人进行对话。并且呢。这三个人的语音呢还是有交叉的。那我们来看一下这个模型。它识别的情况。好这边呢他就识别完成了。我们可以看到这个识别呢。是完全没有问题。甚至连两个说话人交叉的语音部分呢。识别的也是完全正确。最后呢。我也测试了一下我上期视频中。我自己的一个语音。我这语音文件呢。在使用这个剪映提供的语音识别。进行字幕转录的时候呢。它里面呢。很多这个英文单词识别呢。都是不正确的。我们来看一下微软的这款模型。识别的怎么样。我提供的这个文件呢。它的长度呢是一分钟的时间。但是呢。我们这里可以看到这个模型呢。它只识别了前30秒。也就是说呢。这个模型单词处理呢。只处理30秒的语音。这一点呢。与这个openAI的Whisper呢是一样的。从这个识别结果来看呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/RZAJaKL-pW4_398.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除了OpenR1这个地方识别错误之外呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其他的地方呢。识别的是完全没有问题。这边呢是后30秒的识别结果。这边呢这个llama识别的是不正确的。其他的地方呢。基本上识别的没有任何问题。相比剪映提供的这个语音识别功能呢。这个模型的识别效果还是非常不错的。在剪映提供的识别功能当中呢。像deepseek llama千问这些地方呢。经常呢识别错误。最后呢我们再来运行一下这个图像。理解的例子。这边呢是图像的地址。然后这个提示词呢是问这个大模型。这个图像中呢有什么好。我们来运行一下这本书呢。这个图像中呢。是在一个建筑前面有一个停止标记。然后我们再来试一下这个图片。我们翻一下来看看它识别的怎么样。他说呢。图片显示两名男子呢坐在户外交谈。左侧的男子呢手持一台平板电脑。似乎呢正在用手势表达。右侧的男子呢。也在用手势交流。他们坐在一张十字长凳上。背景呢是一个花园。背景中呢还有一朵石墙。上面长着一些植物和一颗仙人掌。从这个结果来看呢。大部分的内容呢还是正确的。但是里面呢也有一些错误的地方。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/RZAJaKL-pW4_476.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如说。左侧的男子似乎正在用手势交流。这一点呢。他解释的呢是不正确的。然后后面背景当中的植物呢。也不像是仙人掌。看来这个模型对图像的这种描述。理解能力呢。还是有待提高的。不过他的这个语音识别能力。至少比我现在常用的这个剪映提供。的语音识别能力的还是强不少的。并且这个模型呢只有5.6个币的参数量。正常使用的话呢。大约需要13个g左右的现存。对于语音识别和语音翻译。有需求的小伙伴呢。推荐大家来测试一下这个模型。好以上呢。就是今天的分享。如果你觉得本视频对你有帮助。记得一键三连。感谢收看。我们下期视频再见。</p>

<hr style="clear:both" />
====
<p>https://www.youtube.com/watch?v=RZAJaKL-pW4</p><p>微软最新发布了一款多模态模型，在语音识别任务上表现惊人，成功超越英伟达的Canary-1B，登顶Hugging Face ASR排行榜！本期视频，我们将介绍这个模型的亮点，并实测它的语音识别能力，看看它到底有多强！</p>]]>
      </itunes:summary>
      <description>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><h1>介紹微软 Phi-4 多模態模型</h1>這部影片介紹了微軟最新發布的 Phi-4 Multi model instruct 模型，一款輕量級、開放的多模態基礎模型。影片重點在於其在語音任務上的優異表現，特別是在Huggingface的語音識別模型排行榜上，超越了NVIDIA的Canary 1B 模型。<h1>模型概述</h1>Phi-4 是一個輕量級的多模態基礎模型，基於 Phi3.5 和 Phi4 版本開發。它支援文本、圖像和音訊輸入，並可以生成文本輸出，上下文長度可達 128K tokens。模型的訓練採用了目前常用的技術。在多模態支援語言方面，文本支援多種語言，視覺僅支援英文，音訊則支援英文、中文等 8 種語言。<h1>應用場景與特色</h1>由於 Phi-4 的參數量只有 5.6B，因此更適合計算環境受限或低延遲的場景。 儘管參數量不大，但其能力毫不遜色，包括強大的推理能力（數學和邏輯推理）、工具調用支援、圖像處理能力（圖像理解、OCR 處理、多圖像比較）和音訊處理能力（語音識別、語音問答、翻譯，甚至直接生成語音摘要）。<h1>模型改進與性能</h1>Phi-4 是基於 Phi3 系列開發的版本，主要改進包括新架構設計、更大的詞彙表、多語言和多模態支援。 在性能方面，尤其是在語音識別和翻譯方面，Phi-4 的表現優異。 語音識別誤識率低於 Qwen2-Audio 和 OpenAI 的 Whisper V3。 在語音翻譯方面，性能也超越了其他模型。此外，Phi-4 還支援視覺和語音的同步輸入，在多個測試基準中表現出色。<h1>模型訓練細節</h1>Phi-4 的訓練使用了 512 張 A100 進行，訓練時間為 28 天。 訓練資料包括 5T tokens 的文本資料、230 萬小時的語音資料，以及 1.1T 的圖像和文本對齊資料。 影片中也介紹了訓練數據的組成以及數據的篩選和處理方式。<h1>模型使用示範</h1>為了使用 Phi-4 模型，首先需要安裝運行環境並安裝相關的開發庫。 影片演示了如何下載模型並加載到本地機器上，模型的顯存占用約為 13GB。 接下來，影片展示了語音識別和翻譯的測試，使用微軟官方的例子，將語音識別為文本，然後翻譯成中文，並在識別文本和翻譯結果之間加上 Sep 標記。<h1>語音識別能力測試</h1>為了測試 Phi-4 的語音識別能力，影片使用了多人對話的語音進行測試，結果顯示模型可以準確識別，甚至可以正確識別兩個說話者交叉的部分。 此外，影片還測試了使用剪映提供的語音識別功能時，在辨識一些英文單字時經常出現錯誤的影片，Phi-4 的表現要好得多，除了 OpenR1 辨識錯誤外，其他地方基本準確，相較於剪映，Phi-4 的識別效果更佳。<h1>圖像理解能力測試</h1>影片還演示了圖像理解的例子，詢問模型圖像中有什麼內容。 雖然模型對圖像的描述理解能力有待提高，但總體來說，Phi-4 的語音識別能力優於剪映提供的語音識別功能。 尤其是在模型只有 5.6B 參數，正常使用僅需 13GB 顯存的情況下，對於有語音識別和翻譯需求的使用者，推薦測試此模型。<hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">AI开发前途无限。欢迎来到AI开发者的频道。今天这个视频呢。我们介绍一下。微软这周刚刚发布的一款最新的模型。也就是这个Phi-4 Multi model instruct模型。这是一款轻量级的开放的。多模态的基础模型。之所以介绍这款模型呢。主要是因为这款模型。在语音任务上的表现。可以说是非常的棒。它在Huggingface的。语音识别模型排行榜上呢。超过了当前排名第一的。英伟达的Canary 1B的模型。下面呢首先我会介绍一下这款模型。然后呢。再来介绍一下如何使用这个模型。并来测试一下它的语音识别能力。好首先我们先来看一下这个模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那这个模型呢。它是一个轻量级的多模态基础模型。它是在Phi3.5和Phi4版本的基础上的。来建立起来的。这款模型呢。它支持处理文本。图像和音频的输入。并可以生成文本的输出。它支持呢128K TOKEN的上下文。模型的训练呢。采用的这些技术呢。是当前最常用的一些技术。这里呢。是它各个模态支持的语言的情况。文本呢。我们可以看到它支持的语言呢。还是挺多的。视觉呢只支持英文。音频呢支持英文。中文等8种语言。然后呢是微软介绍的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个模型可以适用的应用场景。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/RZAJaKL-pW4_79.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先呢。这个模型的参数量呢只有5.6个B。所以呢。它更适合于这种计算环境受限。或者低延迟的这种场景。虽然参数量不大。但是他的能力呢。一点也不差。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如他的强推理能力。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">也就是说他的数学。和逻辑推理能力呢是不错的。然后呢他还可以支持呢使用工具调用。下面呢就是它的这个图像处理能力。包括图像理解。OCR的处理。多图像的对比等等。最后呢是它的这个音频的处理能力。包括语音识别。语音的问答翻译。甚至呢可以对语音做直接的摘要。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三点呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是当前这个开放版本的说明。这里他讲呢。他这个版本呢是基于Phi3系列来建立的。相比Phi3系列呢。它主要的改进呢有新架构的设计。更大的词汇表。多语言和多模态的支持等等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第四点呢是模型的性能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">也就是呢。这个模型在公开测试集上的表现。到底如何。首先呢。我们先看一下它的这个语音能力。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/RZAJaKL-pW4_142.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一点呢。就是说。它的这个语音识别和语音翻译的能力。是非常棒的。这个表呢是语音识别上的一个结果。通过这些结果我们可以看到。这个模型呢在语音识别上的误识率呢。低于Qwen2-Audio以及open AI的Whisper V3。在下面这个结果呢。是它在8种语言上的语音识别结果。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三个结果呢是语音翻译。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个结果呢是值越高呢性能越好。从这个结果也可以看到。微软的这款模型呢在语音翻译上呢。也超过了当前我们熟知的。这种性能比较好的模型。然后这款模型呢。还可以同时支持。这个视觉和语音的输入。这个结果呢。它是比较了。与当前支持音频和视觉输入的。最好的多模态模型的一个比较。从这个多个测试基准中呢我们可以看。到这个Phi-4多模态的模型。表现还是非常不错的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除了结果呢。微软呢还介绍了。这个模型的训练的一些相关的细节。这个模型训练呢。使用了512张A100进行训练的。训练时间呢是训练了28天。训练数据呢包括5T TOKEN的文本数据。230万小时的语音数据。和这个1.1T的图像和文本对齐数据。虽然说模型不大。但是这个数据量还是很大的。然后呢他还介绍了。他们的这个训练数据的组成。以及这个数据。是如何进行筛选和处理的。好以上内容呢。感兴趣的小伙伴呢。可以去自己读一下。这个模型的技术报告。最后呢。我们来介绍一下如何运行这个模型。要运行这个模型呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/RZAJaKL-pW4_242.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先呢我们需要安装一个运行环境。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">并根据他这的要求呢。安装相关的这种开发库。这部分呢。是我们导入这个相关的开发库。然后执行这一部分的代码呢。就可以将这个模型。下载到我们本地的机器上。这边呢我已经下载好了。点击这个执行之后呢。就可以看到这个模型呢被导入了。这个模型。导入之后呢可以看到这个显存占用呢。大约是在13个g左右。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先呢。我们来测一下它这个语音识别。以及翻译的能力。这个例子呢。是微软官方给出的一个例子。这边呢是一个语音文件。然后呢是这个模型的一个提示词。这个提示词呢是让这个模型呢。将这个语音呢识别为文本。然后呢再翻译为中文。并要求这个模型呢。在识别文本与翻译结果。之间呢加上Sep这个标记。然后这就是读取这个语音的文件。读取之后呢。是做TOKEN和数据的基本处理。然后就是进行这个文本的产生。好我们可以看到这边结果呢。就产生出来了。这上面呢还显示了他的这个提示词。以及这个模型产生的回答。我们可以看到。前面呢是语音识别的部分。Sep后面的部分呢。就是将上面的这个识别的文本。翻译为了中文。然后。为了测试一下它语音识别的能力呢。我找了一个多人对话的语音。我们可以先来听一下这段语音。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/RZAJaKL-pW4_337.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">你可以听到这个语音当中呢。它有多个人进行对话。并且呢。这三个人的语音呢还是有交叉的。那我们来看一下这个模型。它识别的情况。好这边呢他就识别完成了。我们可以看到这个识别呢。是完全没有问题。甚至连两个说话人交叉的语音部分呢。识别的也是完全正确。最后呢。我也测试了一下我上期视频中。我自己的一个语音。我这语音文件呢。在使用这个剪映提供的语音识别。进行字幕转录的时候呢。它里面呢。很多这个英文单词识别呢。都是不正确的。我们来看一下微软的这款模型。识别的怎么样。我提供的这个文件呢。它的长度呢是一分钟的时间。但是呢。我们这里可以看到这个模型呢。它只识别了前30秒。也就是说呢。这个模型单词处理呢。只处理30秒的语音。这一点呢。与这个openAI的Whisper呢是一样的。从这个识别结果来看呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/RZAJaKL-pW4_398.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除了OpenR1这个地方识别错误之外呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其他的地方呢。识别的是完全没有问题。这边呢是后30秒的识别结果。这边呢这个llama识别的是不正确的。其他的地方呢。基本上识别的没有任何问题。相比剪映提供的这个语音识别功能呢。这个模型的识别效果还是非常不错的。在剪映提供的识别功能当中呢。像deepseek llama千问这些地方呢。经常呢识别错误。最后呢我们再来运行一下这个图像。理解的例子。这边呢是图像的地址。然后这个提示词呢是问这个大模型。这个图像中呢有什么好。我们来运行一下这本书呢。这个图像中呢。是在一个建筑前面有一个停止标记。然后我们再来试一下这个图片。我们翻一下来看看它识别的怎么样。他说呢。图片显示两名男子呢坐在户外交谈。左侧的男子呢手持一台平板电脑。似乎呢正在用手势表达。右侧的男子呢。也在用手势交流。他们坐在一张十字长凳上。背景呢是一个花园。背景中呢还有一朵石墙。上面长着一些植物和一颗仙人掌。从这个结果来看呢。大部分的内容呢还是正确的。但是里面呢也有一些错误的地方。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/RZAJaKL-pW4_476.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如说。左侧的男子似乎正在用手势交流。这一点呢。他解释的呢是不正确的。然后后面背景当中的植物呢。也不像是仙人掌。看来这个模型对图像的这种描述。理解能力呢。还是有待提高的。不过他的这个语音识别能力。至少比我现在常用的这个剪映提供。的语音识别能力的还是强不少的。并且这个模型呢只有5.6个币的参数量。正常使用的话呢。大约需要13个g左右的现存。对于语音识别和语音翻译。有需求的小伙伴呢。推荐大家来测试一下这个模型。好以上呢。就是今天的分享。如果你觉得本视频对你有帮助。记得一键三连。感谢收看。我们下期视频再见。</p>

<hr style="clear:both" />
====
<p>https://www.youtube.com/watch?v=RZAJaKL-pW4</p><p>微软最新发布了一款多模态模型，在语音识别任务上表现惊人，成功超越英伟达的Canary-1B，登顶Hugging Face ASR排行榜！本期视频，我们将介绍这个模型的亮点，并实测它的语音识别能力，看看它到底有多强！</p>]]>
      </description>
      <content:encoded><![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><h1>介紹微软 Phi-4 多模態模型</h1>這部影片介紹了微軟最新發布的 Phi-4 Multi model instruct 模型，一款輕量級、開放的多模態基礎模型。影片重點在於其在語音任務上的優異表現，特別是在Huggingface的語音識別模型排行榜上，超越了NVIDIA的Canary 1B 模型。<h1>模型概述</h1>Phi-4 是一個輕量級的多模態基礎模型，基於 Phi3.5 和 Phi4 版本開發。它支援文本、圖像和音訊輸入，並可以生成文本輸出，上下文長度可達 128K tokens。模型的訓練採用了目前常用的技術。在多模態支援語言方面，文本支援多種語言，視覺僅支援英文，音訊則支援英文、中文等 8 種語言。<h1>應用場景與特色</h1>由於 Phi-4 的參數量只有 5.6B，因此更適合計算環境受限或低延遲的場景。 儘管參數量不大，但其能力毫不遜色，包括強大的推理能力（數學和邏輯推理）、工具調用支援、圖像處理能力（圖像理解、OCR 處理、多圖像比較）和音訊處理能力（語音識別、語音問答、翻譯，甚至直接生成語音摘要）。<h1>模型改進與性能</h1>Phi-4 是基於 Phi3 系列開發的版本，主要改進包括新架構設計、更大的詞彙表、多語言和多模態支援。 在性能方面，尤其是在語音識別和翻譯方面，Phi-4 的表現優異。 語音識別誤識率低於 Qwen2-Audio 和 OpenAI 的 Whisper V3。 在語音翻譯方面，性能也超越了其他模型。此外，Phi-4 還支援視覺和語音的同步輸入，在多個測試基準中表現出色。<h1>模型訓練細節</h1>Phi-4 的訓練使用了 512 張 A100 進行，訓練時間為 28 天。 訓練資料包括 5T tokens 的文本資料、230 萬小時的語音資料，以及 1.1T 的圖像和文本對齊資料。 影片中也介紹了訓練數據的組成以及數據的篩選和處理方式。<h1>模型使用示範</h1>為了使用 Phi-4 模型，首先需要安裝運行環境並安裝相關的開發庫。 影片演示了如何下載模型並加載到本地機器上，模型的顯存占用約為 13GB。 接下來，影片展示了語音識別和翻譯的測試，使用微軟官方的例子，將語音識別為文本，然後翻譯成中文，並在識別文本和翻譯結果之間加上 Sep 標記。<h1>語音識別能力測試</h1>為了測試 Phi-4 的語音識別能力，影片使用了多人對話的語音進行測試，結果顯示模型可以準確識別，甚至可以正確識別兩個說話者交叉的部分。 此外，影片還測試了使用剪映提供的語音識別功能時，在辨識一些英文單字時經常出現錯誤的影片，Phi-4 的表現要好得多，除了 OpenR1 辨識錯誤外，其他地方基本準確，相較於剪映，Phi-4 的識別效果更佳。<h1>圖像理解能力測試</h1>影片還演示了圖像理解的例子，詢問模型圖像中有什麼內容。 雖然模型對圖像的描述理解能力有待提高，但總體來說，Phi-4 的語音識別能力優於剪映提供的語音識別功能。 尤其是在模型只有 5.6B 參數，正常使用僅需 13GB 顯存的情況下，對於有語音識別和翻譯需求的使用者，推薦測試此模型。<hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">AI开发前途无限。欢迎来到AI开发者的频道。今天这个视频呢。我们介绍一下。微软这周刚刚发布的一款最新的模型。也就是这个Phi-4 Multi model instruct模型。这是一款轻量级的开放的。多模态的基础模型。之所以介绍这款模型呢。主要是因为这款模型。在语音任务上的表现。可以说是非常的棒。它在Huggingface的。语音识别模型排行榜上呢。超过了当前排名第一的。英伟达的Canary 1B的模型。下面呢首先我会介绍一下这款模型。然后呢。再来介绍一下如何使用这个模型。并来测试一下它的语音识别能力。好首先我们先来看一下这个模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那这个模型呢。它是一个轻量级的多模态基础模型。它是在Phi3.5和Phi4版本的基础上的。来建立起来的。这款模型呢。它支持处理文本。图像和音频的输入。并可以生成文本的输出。它支持呢128K TOKEN的上下文。模型的训练呢。采用的这些技术呢。是当前最常用的一些技术。这里呢。是它各个模态支持的语言的情况。文本呢。我们可以看到它支持的语言呢。还是挺多的。视觉呢只支持英文。音频呢支持英文。中文等8种语言。然后呢是微软介绍的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个模型可以适用的应用场景。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/RZAJaKL-pW4_79.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先呢。这个模型的参数量呢只有5.6个B。所以呢。它更适合于这种计算环境受限。或者低延迟的这种场景。虽然参数量不大。但是他的能力呢。一点也不差。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如他的强推理能力。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">也就是说他的数学。和逻辑推理能力呢是不错的。然后呢他还可以支持呢使用工具调用。下面呢就是它的这个图像处理能力。包括图像理解。OCR的处理。多图像的对比等等。最后呢是它的这个音频的处理能力。包括语音识别。语音的问答翻译。甚至呢可以对语音做直接的摘要。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三点呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是当前这个开放版本的说明。这里他讲呢。他这个版本呢是基于Phi3系列来建立的。相比Phi3系列呢。它主要的改进呢有新架构的设计。更大的词汇表。多语言和多模态的支持等等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第四点呢是模型的性能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">也就是呢。这个模型在公开测试集上的表现。到底如何。首先呢。我们先看一下它的这个语音能力。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/RZAJaKL-pW4_142.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一点呢。就是说。它的这个语音识别和语音翻译的能力。是非常棒的。这个表呢是语音识别上的一个结果。通过这些结果我们可以看到。这个模型呢在语音识别上的误识率呢。低于Qwen2-Audio以及open AI的Whisper V3。在下面这个结果呢。是它在8种语言上的语音识别结果。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三个结果呢是语音翻译。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个结果呢是值越高呢性能越好。从这个结果也可以看到。微软的这款模型呢在语音翻译上呢。也超过了当前我们熟知的。这种性能比较好的模型。然后这款模型呢。还可以同时支持。这个视觉和语音的输入。这个结果呢。它是比较了。与当前支持音频和视觉输入的。最好的多模态模型的一个比较。从这个多个测试基准中呢我们可以看。到这个Phi-4多模态的模型。表现还是非常不错的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除了结果呢。微软呢还介绍了。这个模型的训练的一些相关的细节。这个模型训练呢。使用了512张A100进行训练的。训练时间呢是训练了28天。训练数据呢包括5T TOKEN的文本数据。230万小时的语音数据。和这个1.1T的图像和文本对齐数据。虽然说模型不大。但是这个数据量还是很大的。然后呢他还介绍了。他们的这个训练数据的组成。以及这个数据。是如何进行筛选和处理的。好以上内容呢。感兴趣的小伙伴呢。可以去自己读一下。这个模型的技术报告。最后呢。我们来介绍一下如何运行这个模型。要运行这个模型呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/RZAJaKL-pW4_242.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先呢我们需要安装一个运行环境。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">并根据他这的要求呢。安装相关的这种开发库。这部分呢。是我们导入这个相关的开发库。然后执行这一部分的代码呢。就可以将这个模型。下载到我们本地的机器上。这边呢我已经下载好了。点击这个执行之后呢。就可以看到这个模型呢被导入了。这个模型。导入之后呢可以看到这个显存占用呢。大约是在13个g左右。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先呢。我们来测一下它这个语音识别。以及翻译的能力。这个例子呢。是微软官方给出的一个例子。这边呢是一个语音文件。然后呢是这个模型的一个提示词。这个提示词呢是让这个模型呢。将这个语音呢识别为文本。然后呢再翻译为中文。并要求这个模型呢。在识别文本与翻译结果。之间呢加上Sep这个标记。然后这就是读取这个语音的文件。读取之后呢。是做TOKEN和数据的基本处理。然后就是进行这个文本的产生。好我们可以看到这边结果呢。就产生出来了。这上面呢还显示了他的这个提示词。以及这个模型产生的回答。我们可以看到。前面呢是语音识别的部分。Sep后面的部分呢。就是将上面的这个识别的文本。翻译为了中文。然后。为了测试一下它语音识别的能力呢。我找了一个多人对话的语音。我们可以先来听一下这段语音。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/RZAJaKL-pW4_337.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">你可以听到这个语音当中呢。它有多个人进行对话。并且呢。这三个人的语音呢还是有交叉的。那我们来看一下这个模型。它识别的情况。好这边呢他就识别完成了。我们可以看到这个识别呢。是完全没有问题。甚至连两个说话人交叉的语音部分呢。识别的也是完全正确。最后呢。我也测试了一下我上期视频中。我自己的一个语音。我这语音文件呢。在使用这个剪映提供的语音识别。进行字幕转录的时候呢。它里面呢。很多这个英文单词识别呢。都是不正确的。我们来看一下微软的这款模型。识别的怎么样。我提供的这个文件呢。它的长度呢是一分钟的时间。但是呢。我们这里可以看到这个模型呢。它只识别了前30秒。也就是说呢。这个模型单词处理呢。只处理30秒的语音。这一点呢。与这个openAI的Whisper呢是一样的。从这个识别结果来看呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/RZAJaKL-pW4_398.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除了OpenR1这个地方识别错误之外呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其他的地方呢。识别的是完全没有问题。这边呢是后30秒的识别结果。这边呢这个llama识别的是不正确的。其他的地方呢。基本上识别的没有任何问题。相比剪映提供的这个语音识别功能呢。这个模型的识别效果还是非常不错的。在剪映提供的识别功能当中呢。像deepseek llama千问这些地方呢。经常呢识别错误。最后呢我们再来运行一下这个图像。理解的例子。这边呢是图像的地址。然后这个提示词呢是问这个大模型。这个图像中呢有什么好。我们来运行一下这本书呢。这个图像中呢。是在一个建筑前面有一个停止标记。然后我们再来试一下这个图片。我们翻一下来看看它识别的怎么样。他说呢。图片显示两名男子呢坐在户外交谈。左侧的男子呢手持一台平板电脑。似乎呢正在用手势表达。右侧的男子呢。也在用手势交流。他们坐在一张十字长凳上。背景呢是一个花园。背景中呢还有一朵石墙。上面长着一些植物和一颗仙人掌。从这个结果来看呢。大部分的内容呢还是正确的。但是里面呢也有一些错误的地方。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/RZAJaKL-pW4_476.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如说。左侧的男子似乎正在用手势交流。这一点呢。他解释的呢是不正确的。然后后面背景当中的植物呢。也不像是仙人掌。看来这个模型对图像的这种描述。理解能力呢。还是有待提高的。不过他的这个语音识别能力。至少比我现在常用的这个剪映提供。的语音识别能力的还是强不少的。并且这个模型呢只有5.6个币的参数量。正常使用的话呢。大约需要13个g左右的现存。对于语音识别和语音翻译。有需求的小伙伴呢。推荐大家来测试一下这个模型。好以上呢。就是今天的分享。如果你觉得本视频对你有帮助。记得一键三连。感谢收看。我们下期视频再见。</p>

<hr style="clear:both" />
====
<p>https://www.youtube.com/watch?v=RZAJaKL-pW4</p><p>微软最新发布了一款多模态模型，在语音识别任务上表现惊人，成功超越英伟达的Canary-1B，登顶Hugging Face ASR排行榜！本期视频，我们将介绍这个模型的亮点，并实测它的语音识别能力，看看它到底有多强！</p>]]></content:encoded>
      <itunes:image href="https://i.ytimg.com/vi/RZAJaKL-pW4/hqdefault.jpg"/>
      <pubDate>2025-03-01T13:37:12.000Z</pubDate>
    </item><item>
      <title><![CDATA[训练推理模型必备！Math-200K数据集制作全解析！]]></title>
      <link>https://www.youtube.com/watch?v=qlR3XddFhuY</link>
      <itunes:title><![CDATA[训练推理模型必备！Math-200K数据集制作全解析！]]></itunes:title>
      <itunes:author><![CDATA[AI开发者-就爱瞎鼓捣]]></itunes:author>
      <itunes:summary>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">好的，以下是影片的摘要：<h1>引言：AI推理数据的重要性</h1>影片开篇介绍了 AI 开发领域中推理数据的重要性。特别是，对于那些希望在特定任务上训练推理模型的人来说，高质量的推理数据至关重要。 影片以 OpenR1 项目为例，详细介绍了如何制作推理数据。 OpenR1 最近发布了 Math 220k 数据集，为制作推理数据集提供了有价值的参考。<h1>DeepSeek 的启发与 OpenR1 的贡献</h1>影片提到 DeepSeek 团队通过使用 60 万条推理训练数据，大幅提升了 Qwen 和 Llama 等开源模型的性能，但 DeepSeek 并未公开其数据集的制作方法。OpenR1 项目填补了这个空白，公开了 Math 220k 数据集的详细制作方法，为其他人提供了可复制的实践经验。<h1>数据集概览与成本考量</h1>影片概述了 Math 220k 数据集的主要特征：它包含了 80 万条推理轨迹，这些轨迹是 DeepSeek R1 模型针对 40 万个问题生成的，经过筛选后形成了 22 万条正确的推理轨迹。影片也提到了数据集的制作成本，使用了 512 张 H100 显卡，大约需要 4 天时间来生成数据，这显示了制作推理数据所需的计算资源。<h1>数据集的验证与结果</h1>OpenR1 使用 math verify 工具筛选答案，并用 Llama3.3-70B 模型进一步评审，筛选出更准确的推理轨迹。最终，使用该数据集微调的 Qwen 7B math 模型，其性能与 DeepSeek 微调的 Qwen 7B 模型非常接近，验证了 OpenR1 数据集的质量。<h1>数据集的制作方法：数据生成</h1>第一步是生成数据。 使用 DeepSeek R1 模型解答来自现有数据集的 40 万个问题，并展示了使用的提示词。 影片还提到了最大上下文长度（token）的设置，设置成 16k 能够解决更多问题。 影片还比较了 vLLM 和 SGLang 这两个本地部署工具的推理速度，SGLang 更快。<h1>数据集的制作方法：数据筛选</h1>第二步是筛选数据。 OpenR1 使用了两种筛选方法：基于规则的 Math verify 和基于模型的 Llama3.3-70B 模型的评审。Math verify 基于答案的正确性进行筛选，而 Llama 模型则进一步筛选 Math verify 遗漏的正确答案。<h1>模型微调结果与未来展望</h1>影片展示了使用 Math 220k 数据集微调的 Qwen 2.5 Math instruct 模型的性能，并与 DeepSeek 的模型进行了比较。 虽然在 MATH500 和 AIME25 上性能接近，但在 AIME24 上存在差距，但考虑到 OpenR1 是首次尝试，改进空间巨大。影片提到了未来改进的可能性，如增加问题集和优化筛选策略。 影片最后分享了自己使用该数据集微调 Qwen 1.5B 模型的经验，并鼓励观众关注后续的 OpenR1 项目进展。<hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">AI开发前途无限。欢迎来到AI开发者的频道。今天我们来看一下OpenR1项目中。他们是如何来制作推理数据的。制作推理数据呢。对于要在自己任务上。制作推理模型的小伙伴来说呢。可以说是非常的重要。下面我们就基于这个OpenR1最新发布的。这个Math 220k。这个数据集。来看看呢。我们应该如何来制作这种推理数据。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先我们先来聊一下。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么要制作这么一个数据集。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们知道DeepSeek团队呢。最近公开了一系列的。这种distillation的蒸馏模型。通过这些模型。我们就可以知道DeepSeek呢。通过使用60万条推理训练数据呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他将这个Qwen。Llama等开源模型的性能。在不使用强化学习的情况下。就可以大幅度的提升他们的表现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">根据DeepSeek的这个结果来看呢。从道理上来说呢。只要我们有推理数据集。那么我们就可以在自己的任务上。构建这种高性能的推理模型了。但可惜的是呢。DeepSeek它并没有公开这种数据集。是如何制作的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/qlR3XddFhuY_64.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">OpenR1这个项目呢。他们呢最近就公开了。他们的这个Math 220k的数据集的。一个详细的制作方法。所以如果你要制作。这种推理模型的训练数据的话呢。就可以参考他的这个做法。在介绍这个制作方法之前呢。我们先来看一下这个数据集。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先这个数据集呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他有80万条R1的推理轨迹。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这推理轨迹呢。是使用这个DeepSeek R1模型针对40万个问题呢。每个问题生成两。个答案最后呢。又经过筛选。最后形成了22万条的。这个正确的推理轨迹。这个数据集呢。Huggingface使用的是512张H100。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在本地来进行运行。这个DeepSeek模型。本地部署工具呢。它使用的是vLLM和SGLang。那这80万条推理轨迹呢。可能大约需要4天的时间。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">512张H100 四天的时间。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这样来看的话。这个推理数据的制作费用还是挺高的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它这个数据集呢是一个数学问题集。这些问题呢。都来自于这个数据集。数据集推理完成之后呢。它使用的是这个math verify。来选择一个正确的答案。然后呢又进一步。使用。llama3.3-70B的这个模型的做评审工具。来进一步检索这个正确的推理轨迹。最后这一点呢。是使用这个数据集。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/qlR3XddFhuY_146.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">微调了这个Qwen 7B math这个模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">微调之后的模型呢。它的性能呢。与DeepSeek微调的。这个Qwen7B的模型的性能呢。是非常接近的。通过这个性能的这个比较呢。就可以知道。至少在这个数学的任务上。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">OpenR1制作的这个推理数据集呢。质量呢还是不错的。好下面咱们来详细介绍一下。它这个数据集是如何制作的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先第一步呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">是数据的生成。他为了构建这个OpenR1220K这个数据集呢。它使用的是DeepSeek R1的模型。然后呢针对这个数据集的40万个问题。生成解答。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这里呢。他进一步给出了他使用的这个提示词。再下面一点呢。就是这个最大商城TOKEN的设置。这里呢它设置为。16k他这里讲呢当设置为8k的时候呢。只有75%的问题可以被解决。也就是说呢。还有25%的问题是需要16k的设置的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">前面我们讲过。它使用了这个vLLM和这个SGLang。这这两款本地部署工具呢做推理。SGLang这个工具呢。要比vLLM的这个推理速度。要快不少呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/qlR3XddFhuY_216.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">从这里可以看出。使用vLLM时呢。每张H100呢每小时可以生成15个解答。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是使用这个SGLang的时候呢。它每个小时可以生成25个解答。第二步呢就是数据的筛选。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它这边呢数据筛选呢使用了两种方法。第一种呢是基于规则的这个筛选方式。也就是使用这个Math verify的筛选。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">因为它这个数据集吧是一个数学题。我们这道数学题呢。一般都是有标准答案的。所以它这个工具呢。就是比较这个模型的解答。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">与标准答案呢。是不是一致。因为模型输出的格式的影响呢。这个math verify这个工具呢。可能并不能完全的检测出。这个正确的答案。所以呢他们还进一步使用这个。llama 3.3 70B这个模型的。作为评审模型。来进一步进行筛选。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">通过使用这个模型进行筛选呢。他又进一步恢复了。2.8万个被拒绝的问题。这下面呢。就是他使用这个模型进行筛选时。使用的一个提示词。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/qlR3XddFhuY_276.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后呢通过这两种方式吧。在生成的80万个推理轨迹当中呢。他一共保留了22万个推理轨迹。来作为最终的这个数据集。好最后我们再来看一下他使用。这个数据集微调的模型的结果如何。微调的模型呢是qwen 2.5 Math instruct。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一共呢进行了三轮的微调。使用的这个数据集呢。是这个default的部分。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">也就是呢大约9.4万个问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这里呢还列出了一些他训练的。一些相关的一些参数的设置。最后我们可以看一下这个结果。第一列呢是DeepSeek发布的这个模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二点呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是这个open R1项目组。使用这个数据集微调的一款模型。从这个结果可以看出。在这个MATH500和这个AIME25上呢。这个结果呢还是非常的接近的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是在这个AIME 24上呢。这个结果的差距呢。还是稍微有一些大的。那这个数据集呢。毕竟是这个open R1这个项目组呢。他们的第一次尝试。后续呢应该有很大的一个改进空间。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/qlR3XddFhuY_342.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如说可以进一步增加。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">使用其他的这种问题集。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">生成这个推理的轨迹。也可以呢进一步优化这个筛选的策略。比如说使用更好的模型来进行筛选。相信呢。这个制作的过程会越来越完善。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最近呢我也在使用这个数据集微调。这个Qwen1.5B的一个模型。简单尝试了一下。使用这个数据集微调之后的模型。性能呢还是有一个很明显的改善的。今后我们频道会持续的跟进。这个OpenR1的项目。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">应跟着他们呢。做一些相关的这个验证和实验。如果你对这个项目感兴趣的话呢。可以关注我们的共学群。来一起学习如何浮现DeepSeek R1模型。好的以上就。是今天的分享。如果你觉得本视频对你有帮助。记得一键三连。感谢收看。我们下期视频再见。</p>

<hr style="clear:both" />
====
<p>https://www.youtube.com/watch?v=qlR3XddFhuY</p><p>本期我们继续学习OpenR1项目，深入解析最新发布的Math-200K数据集。了解这个数据集的制作过程，对于想要微调推理模型的开发者来说至关重要！一起来看看它是如何构建的吧！</p>]]>
      </itunes:summary>
      <description>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">好的，以下是影片的摘要：<h1>引言：AI推理数据的重要性</h1>影片开篇介绍了 AI 开发领域中推理数据的重要性。特别是，对于那些希望在特定任务上训练推理模型的人来说，高质量的推理数据至关重要。 影片以 OpenR1 项目为例，详细介绍了如何制作推理数据。 OpenR1 最近发布了 Math 220k 数据集，为制作推理数据集提供了有价值的参考。<h1>DeepSeek 的启发与 OpenR1 的贡献</h1>影片提到 DeepSeek 团队通过使用 60 万条推理训练数据，大幅提升了 Qwen 和 Llama 等开源模型的性能，但 DeepSeek 并未公开其数据集的制作方法。OpenR1 项目填补了这个空白，公开了 Math 220k 数据集的详细制作方法，为其他人提供了可复制的实践经验。<h1>数据集概览与成本考量</h1>影片概述了 Math 220k 数据集的主要特征：它包含了 80 万条推理轨迹，这些轨迹是 DeepSeek R1 模型针对 40 万个问题生成的，经过筛选后形成了 22 万条正确的推理轨迹。影片也提到了数据集的制作成本，使用了 512 张 H100 显卡，大约需要 4 天时间来生成数据，这显示了制作推理数据所需的计算资源。<h1>数据集的验证与结果</h1>OpenR1 使用 math verify 工具筛选答案，并用 Llama3.3-70B 模型进一步评审，筛选出更准确的推理轨迹。最终，使用该数据集微调的 Qwen 7B math 模型，其性能与 DeepSeek 微调的 Qwen 7B 模型非常接近，验证了 OpenR1 数据集的质量。<h1>数据集的制作方法：数据生成</h1>第一步是生成数据。 使用 DeepSeek R1 模型解答来自现有数据集的 40 万个问题，并展示了使用的提示词。 影片还提到了最大上下文长度（token）的设置，设置成 16k 能够解决更多问题。 影片还比较了 vLLM 和 SGLang 这两个本地部署工具的推理速度，SGLang 更快。<h1>数据集的制作方法：数据筛选</h1>第二步是筛选数据。 OpenR1 使用了两种筛选方法：基于规则的 Math verify 和基于模型的 Llama3.3-70B 模型的评审。Math verify 基于答案的正确性进行筛选，而 Llama 模型则进一步筛选 Math verify 遗漏的正确答案。<h1>模型微调结果与未来展望</h1>影片展示了使用 Math 220k 数据集微调的 Qwen 2.5 Math instruct 模型的性能，并与 DeepSeek 的模型进行了比较。 虽然在 MATH500 和 AIME25 上性能接近，但在 AIME24 上存在差距，但考虑到 OpenR1 是首次尝试，改进空间巨大。影片提到了未来改进的可能性，如增加问题集和优化筛选策略。 影片最后分享了自己使用该数据集微调 Qwen 1.5B 模型的经验，并鼓励观众关注后续的 OpenR1 项目进展。<hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">AI开发前途无限。欢迎来到AI开发者的频道。今天我们来看一下OpenR1项目中。他们是如何来制作推理数据的。制作推理数据呢。对于要在自己任务上。制作推理模型的小伙伴来说呢。可以说是非常的重要。下面我们就基于这个OpenR1最新发布的。这个Math 220k。这个数据集。来看看呢。我们应该如何来制作这种推理数据。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先我们先来聊一下。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么要制作这么一个数据集。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们知道DeepSeek团队呢。最近公开了一系列的。这种distillation的蒸馏模型。通过这些模型。我们就可以知道DeepSeek呢。通过使用60万条推理训练数据呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他将这个Qwen。Llama等开源模型的性能。在不使用强化学习的情况下。就可以大幅度的提升他们的表现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">根据DeepSeek的这个结果来看呢。从道理上来说呢。只要我们有推理数据集。那么我们就可以在自己的任务上。构建这种高性能的推理模型了。但可惜的是呢。DeepSeek它并没有公开这种数据集。是如何制作的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/qlR3XddFhuY_64.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">OpenR1这个项目呢。他们呢最近就公开了。他们的这个Math 220k的数据集的。一个详细的制作方法。所以如果你要制作。这种推理模型的训练数据的话呢。就可以参考他的这个做法。在介绍这个制作方法之前呢。我们先来看一下这个数据集。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先这个数据集呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他有80万条R1的推理轨迹。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这推理轨迹呢。是使用这个DeepSeek R1模型针对40万个问题呢。每个问题生成两。个答案最后呢。又经过筛选。最后形成了22万条的。这个正确的推理轨迹。这个数据集呢。Huggingface使用的是512张H100。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在本地来进行运行。这个DeepSeek模型。本地部署工具呢。它使用的是vLLM和SGLang。那这80万条推理轨迹呢。可能大约需要4天的时间。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">512张H100 四天的时间。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这样来看的话。这个推理数据的制作费用还是挺高的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它这个数据集呢是一个数学问题集。这些问题呢。都来自于这个数据集。数据集推理完成之后呢。它使用的是这个math verify。来选择一个正确的答案。然后呢又进一步。使用。llama3.3-70B的这个模型的做评审工具。来进一步检索这个正确的推理轨迹。最后这一点呢。是使用这个数据集。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/qlR3XddFhuY_146.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">微调了这个Qwen 7B math这个模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">微调之后的模型呢。它的性能呢。与DeepSeek微调的。这个Qwen7B的模型的性能呢。是非常接近的。通过这个性能的这个比较呢。就可以知道。至少在这个数学的任务上。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">OpenR1制作的这个推理数据集呢。质量呢还是不错的。好下面咱们来详细介绍一下。它这个数据集是如何制作的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先第一步呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">是数据的生成。他为了构建这个OpenR1220K这个数据集呢。它使用的是DeepSeek R1的模型。然后呢针对这个数据集的40万个问题。生成解答。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这里呢。他进一步给出了他使用的这个提示词。再下面一点呢。就是这个最大商城TOKEN的设置。这里呢它设置为。16k他这里讲呢当设置为8k的时候呢。只有75%的问题可以被解决。也就是说呢。还有25%的问题是需要16k的设置的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">前面我们讲过。它使用了这个vLLM和这个SGLang。这这两款本地部署工具呢做推理。SGLang这个工具呢。要比vLLM的这个推理速度。要快不少呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/qlR3XddFhuY_216.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">从这里可以看出。使用vLLM时呢。每张H100呢每小时可以生成15个解答。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是使用这个SGLang的时候呢。它每个小时可以生成25个解答。第二步呢就是数据的筛选。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它这边呢数据筛选呢使用了两种方法。第一种呢是基于规则的这个筛选方式。也就是使用这个Math verify的筛选。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">因为它这个数据集吧是一个数学题。我们这道数学题呢。一般都是有标准答案的。所以它这个工具呢。就是比较这个模型的解答。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">与标准答案呢。是不是一致。因为模型输出的格式的影响呢。这个math verify这个工具呢。可能并不能完全的检测出。这个正确的答案。所以呢他们还进一步使用这个。llama 3.3 70B这个模型的。作为评审模型。来进一步进行筛选。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">通过使用这个模型进行筛选呢。他又进一步恢复了。2.8万个被拒绝的问题。这下面呢。就是他使用这个模型进行筛选时。使用的一个提示词。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/qlR3XddFhuY_276.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后呢通过这两种方式吧。在生成的80万个推理轨迹当中呢。他一共保留了22万个推理轨迹。来作为最终的这个数据集。好最后我们再来看一下他使用。这个数据集微调的模型的结果如何。微调的模型呢是qwen 2.5 Math instruct。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一共呢进行了三轮的微调。使用的这个数据集呢。是这个default的部分。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">也就是呢大约9.4万个问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这里呢还列出了一些他训练的。一些相关的一些参数的设置。最后我们可以看一下这个结果。第一列呢是DeepSeek发布的这个模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二点呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是这个open R1项目组。使用这个数据集微调的一款模型。从这个结果可以看出。在这个MATH500和这个AIME25上呢。这个结果呢还是非常的接近的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是在这个AIME 24上呢。这个结果的差距呢。还是稍微有一些大的。那这个数据集呢。毕竟是这个open R1这个项目组呢。他们的第一次尝试。后续呢应该有很大的一个改进空间。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/qlR3XddFhuY_342.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如说可以进一步增加。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">使用其他的这种问题集。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">生成这个推理的轨迹。也可以呢进一步优化这个筛选的策略。比如说使用更好的模型来进行筛选。相信呢。这个制作的过程会越来越完善。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最近呢我也在使用这个数据集微调。这个Qwen1.5B的一个模型。简单尝试了一下。使用这个数据集微调之后的模型。性能呢还是有一个很明显的改善的。今后我们频道会持续的跟进。这个OpenR1的项目。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">应跟着他们呢。做一些相关的这个验证和实验。如果你对这个项目感兴趣的话呢。可以关注我们的共学群。来一起学习如何浮现DeepSeek R1模型。好的以上就。是今天的分享。如果你觉得本视频对你有帮助。记得一键三连。感谢收看。我们下期视频再见。</p>

<hr style="clear:both" />
====
<p>https://www.youtube.com/watch?v=qlR3XddFhuY</p><p>本期我们继续学习OpenR1项目，深入解析最新发布的Math-200K数据集。了解这个数据集的制作过程，对于想要微调推理模型的开发者来说至关重要！一起来看看它是如何构建的吧！</p>]]>
      </description>
      <content:encoded><![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">好的，以下是影片的摘要：<h1>引言：AI推理数据的重要性</h1>影片开篇介绍了 AI 开发领域中推理数据的重要性。特别是，对于那些希望在特定任务上训练推理模型的人来说，高质量的推理数据至关重要。 影片以 OpenR1 项目为例，详细介绍了如何制作推理数据。 OpenR1 最近发布了 Math 220k 数据集，为制作推理数据集提供了有价值的参考。<h1>DeepSeek 的启发与 OpenR1 的贡献</h1>影片提到 DeepSeek 团队通过使用 60 万条推理训练数据，大幅提升了 Qwen 和 Llama 等开源模型的性能，但 DeepSeek 并未公开其数据集的制作方法。OpenR1 项目填补了这个空白，公开了 Math 220k 数据集的详细制作方法，为其他人提供了可复制的实践经验。<h1>数据集概览与成本考量</h1>影片概述了 Math 220k 数据集的主要特征：它包含了 80 万条推理轨迹，这些轨迹是 DeepSeek R1 模型针对 40 万个问题生成的，经过筛选后形成了 22 万条正确的推理轨迹。影片也提到了数据集的制作成本，使用了 512 张 H100 显卡，大约需要 4 天时间来生成数据，这显示了制作推理数据所需的计算资源。<h1>数据集的验证与结果</h1>OpenR1 使用 math verify 工具筛选答案，并用 Llama3.3-70B 模型进一步评审，筛选出更准确的推理轨迹。最终，使用该数据集微调的 Qwen 7B math 模型，其性能与 DeepSeek 微调的 Qwen 7B 模型非常接近，验证了 OpenR1 数据集的质量。<h1>数据集的制作方法：数据生成</h1>第一步是生成数据。 使用 DeepSeek R1 模型解答来自现有数据集的 40 万个问题，并展示了使用的提示词。 影片还提到了最大上下文长度（token）的设置，设置成 16k 能够解决更多问题。 影片还比较了 vLLM 和 SGLang 这两个本地部署工具的推理速度，SGLang 更快。<h1>数据集的制作方法：数据筛选</h1>第二步是筛选数据。 OpenR1 使用了两种筛选方法：基于规则的 Math verify 和基于模型的 Llama3.3-70B 模型的评审。Math verify 基于答案的正确性进行筛选，而 Llama 模型则进一步筛选 Math verify 遗漏的正确答案。<h1>模型微调结果与未来展望</h1>影片展示了使用 Math 220k 数据集微调的 Qwen 2.5 Math instruct 模型的性能，并与 DeepSeek 的模型进行了比较。 虽然在 MATH500 和 AIME25 上性能接近，但在 AIME24 上存在差距，但考虑到 OpenR1 是首次尝试，改进空间巨大。影片提到了未来改进的可能性，如增加问题集和优化筛选策略。 影片最后分享了自己使用该数据集微调 Qwen 1.5B 模型的经验，并鼓励观众关注后续的 OpenR1 项目进展。<hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">AI开发前途无限。欢迎来到AI开发者的频道。今天我们来看一下OpenR1项目中。他们是如何来制作推理数据的。制作推理数据呢。对于要在自己任务上。制作推理模型的小伙伴来说呢。可以说是非常的重要。下面我们就基于这个OpenR1最新发布的。这个Math 220k。这个数据集。来看看呢。我们应该如何来制作这种推理数据。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先我们先来聊一下。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么要制作这么一个数据集。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们知道DeepSeek团队呢。最近公开了一系列的。这种distillation的蒸馏模型。通过这些模型。我们就可以知道DeepSeek呢。通过使用60万条推理训练数据呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他将这个Qwen。Llama等开源模型的性能。在不使用强化学习的情况下。就可以大幅度的提升他们的表现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">根据DeepSeek的这个结果来看呢。从道理上来说呢。只要我们有推理数据集。那么我们就可以在自己的任务上。构建这种高性能的推理模型了。但可惜的是呢。DeepSeek它并没有公开这种数据集。是如何制作的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/qlR3XddFhuY_64.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">OpenR1这个项目呢。他们呢最近就公开了。他们的这个Math 220k的数据集的。一个详细的制作方法。所以如果你要制作。这种推理模型的训练数据的话呢。就可以参考他的这个做法。在介绍这个制作方法之前呢。我们先来看一下这个数据集。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先这个数据集呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他有80万条R1的推理轨迹。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这推理轨迹呢。是使用这个DeepSeek R1模型针对40万个问题呢。每个问题生成两。个答案最后呢。又经过筛选。最后形成了22万条的。这个正确的推理轨迹。这个数据集呢。Huggingface使用的是512张H100。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在本地来进行运行。这个DeepSeek模型。本地部署工具呢。它使用的是vLLM和SGLang。那这80万条推理轨迹呢。可能大约需要4天的时间。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">512张H100 四天的时间。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这样来看的话。这个推理数据的制作费用还是挺高的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它这个数据集呢是一个数学问题集。这些问题呢。都来自于这个数据集。数据集推理完成之后呢。它使用的是这个math verify。来选择一个正确的答案。然后呢又进一步。使用。llama3.3-70B的这个模型的做评审工具。来进一步检索这个正确的推理轨迹。最后这一点呢。是使用这个数据集。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/qlR3XddFhuY_146.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">微调了这个Qwen 7B math这个模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">微调之后的模型呢。它的性能呢。与DeepSeek微调的。这个Qwen7B的模型的性能呢。是非常接近的。通过这个性能的这个比较呢。就可以知道。至少在这个数学的任务上。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">OpenR1制作的这个推理数据集呢。质量呢还是不错的。好下面咱们来详细介绍一下。它这个数据集是如何制作的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先第一步呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">是数据的生成。他为了构建这个OpenR1220K这个数据集呢。它使用的是DeepSeek R1的模型。然后呢针对这个数据集的40万个问题。生成解答。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这里呢。他进一步给出了他使用的这个提示词。再下面一点呢。就是这个最大商城TOKEN的设置。这里呢它设置为。16k他这里讲呢当设置为8k的时候呢。只有75%的问题可以被解决。也就是说呢。还有25%的问题是需要16k的设置的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">前面我们讲过。它使用了这个vLLM和这个SGLang。这这两款本地部署工具呢做推理。SGLang这个工具呢。要比vLLM的这个推理速度。要快不少呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/qlR3XddFhuY_216.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">从这里可以看出。使用vLLM时呢。每张H100呢每小时可以生成15个解答。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是使用这个SGLang的时候呢。它每个小时可以生成25个解答。第二步呢就是数据的筛选。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它这边呢数据筛选呢使用了两种方法。第一种呢是基于规则的这个筛选方式。也就是使用这个Math verify的筛选。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">因为它这个数据集吧是一个数学题。我们这道数学题呢。一般都是有标准答案的。所以它这个工具呢。就是比较这个模型的解答。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">与标准答案呢。是不是一致。因为模型输出的格式的影响呢。这个math verify这个工具呢。可能并不能完全的检测出。这个正确的答案。所以呢他们还进一步使用这个。llama 3.3 70B这个模型的。作为评审模型。来进一步进行筛选。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">通过使用这个模型进行筛选呢。他又进一步恢复了。2.8万个被拒绝的问题。这下面呢。就是他使用这个模型进行筛选时。使用的一个提示词。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/qlR3XddFhuY_276.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后呢通过这两种方式吧。在生成的80万个推理轨迹当中呢。他一共保留了22万个推理轨迹。来作为最终的这个数据集。好最后我们再来看一下他使用。这个数据集微调的模型的结果如何。微调的模型呢是qwen 2.5 Math instruct。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一共呢进行了三轮的微调。使用的这个数据集呢。是这个default的部分。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">也就是呢大约9.4万个问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这里呢还列出了一些他训练的。一些相关的一些参数的设置。最后我们可以看一下这个结果。第一列呢是DeepSeek发布的这个模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二点呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是这个open R1项目组。使用这个数据集微调的一款模型。从这个结果可以看出。在这个MATH500和这个AIME25上呢。这个结果呢还是非常的接近的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是在这个AIME 24上呢。这个结果的差距呢。还是稍微有一些大的。那这个数据集呢。毕竟是这个open R1这个项目组呢。他们的第一次尝试。后续呢应该有很大的一个改进空间。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT/file-cache/qlR3XddFhuY_342.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如说可以进一步增加。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">使用其他的这种问题集。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">生成这个推理的轨迹。也可以呢进一步优化这个筛选的策略。比如说使用更好的模型来进行筛选。相信呢。这个制作的过程会越来越完善。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最近呢我也在使用这个数据集微调。这个Qwen1.5B的一个模型。简单尝试了一下。使用这个数据集微调之后的模型。性能呢还是有一个很明显的改善的。今后我们频道会持续的跟进。这个OpenR1的项目。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">应跟着他们呢。做一些相关的这个验证和实验。如果你对这个项目感兴趣的话呢。可以关注我们的共学群。来一起学习如何浮现DeepSeek R1模型。好的以上就。是今天的分享。如果你觉得本视频对你有帮助。记得一键三连。感谢收看。我们下期视频再见。</p>

<hr style="clear:both" />
====
<p>https://www.youtube.com/watch?v=qlR3XddFhuY</p><p>本期我们继续学习OpenR1项目，深入解析最新发布的Math-200K数据集。了解这个数据集的制作过程，对于想要微调推理模型的开发者来说至关重要！一起来看看它是如何构建的吧！</p>]]></content:encoded>
      <itunes:image href="https://i.ytimg.com/vi/qlR3XddFhuY/hqdefault.jpg"/>
      <pubDate>2025-02-28T13:32:03.000Z</pubDate>
    </item></channel>
</rss>